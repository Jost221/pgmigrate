# =====================================================================
# pgmigrate config.yaml
#
# Пример конфигурации для миграции PostgreSQL -> PostgreSQL.
# Предполагается демо-БД airlines со схемой "bookings", но структура
# конфига подходит и для любых других проектов.
#
# Как вызывать:
#   ./pgmigrate.sh plan    --config ./config.yaml
#   ./pgmigrate.sh migrate --config ./config.yaml [--resume] [--backup] [--verbose]
#
# Где:
#   --resume  — продолжить по state.tsv, пропуская DONE-задачи
#   --backup  — перед миграцией сделать pg_dump target-БД (может быть ОЧЕНЬ долго)
#   --verbose — печатать INFO/WARN в консоль (по умолчанию только ERROR + прогресс)
# =====================================================================

# ---------------------------------------------------------------------
# 1. Подключения
# ---------------------------------------------------------------------

source:
  # Conninfo-строка для исходной БД (откуда читаем данные и структуру).
  # Формат такой же, как у psql / libpq.
  # ОБЯЗАТЕЛЬНО адаптируйте host/port/dbname/user/password под свой стенд.
  conninfo: "host=localhost port=5432 dbname=demo_airlines user=user1 password=pass1"

target:
  # Conninfo-строка для целевой БД (куда заливаем данные и структуру).
  # Пользователь должен иметь права на создание схем/таблиц/индексов/констрейнтов
  # и вставку данных.
  conninfo: "host=localhost port=5433 dbname=demo_airlines_migrated user=user2 password=pass2"

# ---------------------------------------------------------------------
# 2. Схемы (какие схемы мигрируем)
# ---------------------------------------------------------------------

schemas:
  # Режим работы со схемами:
  #   all      — мигрировать все пользовательские схемы
  #   include  — только те, что явно перечислены в schemas.include
  #   exclude  — все, кроме перечисленных в schemas.exclude
  mode: include

  # Схемы, которые нужно мигрировать (актуально при mode: include).
  include:
    - bookings

  # Схемы, которые нужно исключить (актуально при mode: all или mode: exclude).
  # В демо ничего не исключаем.
  exclude: []

# ---------------------------------------------------------------------
# 3. Роли
# ---------------------------------------------------------------------

roles:
  # Режим переноса ролей:
  #   all   — выполнить pg_dumpall --roles-only на source и применить на target
  #   none  — роли не трогаем (например, если ими управляет отдельный процесс)
  mode: all

# ---------------------------------------------------------------------
# 4. Таблицы
#
# Здесь управляем тем, какие таблицы мигрируем, как режем на чанки,
# какие фильтры (WHERE), сортировку (ORDER BY) и лимиты используем.
# ---------------------------------------------------------------------

tables:
  # Режим работы с таблицами:
  #   all      — все таблицы из выбранных схем (кроме tables.exclude)
  #   include  — только таблицы из списка tables.include
  #   exclude  — все, кроме списка tables.exclude
  #
  # В демо-примере используем include: явно описываем, какие таблицы
  # участвуют в миграции, и их настройки.
  mode: include

  # ---------- Настройки по умолчанию для всех таблиц ----------

  default:
    # chunk_column:
    #   - пустая строка ""  — не резать таблицу на чанки
    #   - имя integer-колонки — резать по ней (обычно это PK/id)
    chunk_column: ""

    # chunk_size:
    #   - целое число, сколько строк примерно в одном чанке
    #   - используется только если chunk_column != ""
    chunk_size: 500000

    # where:
    #   - дополнительное условие для выборки данных на source
    #   - добавляется в запрос как WHERE <условие>
    #   - оставьте пустым, если нужно переносить все строки
    where: ""

    # order_by:
    #   - порядок выборки строк (ORDER BY ...)
    #   - имеет смысл, если важна стабильная сортировка, например по PK
    order_by: ""

    # limit:
    #   - глобальный лимит строк на таблицу (0 = без лимита)
    #   - если для конкретной таблицы задан свой limit, он перекрывает этот
    #   - при limit > 0 таблица НЕ режется на чанки, создаётся одна DATA-задача с LIMIT
    limit: 0

  # ---------- Индивидуальные настройки для конкретных таблиц ----------

  include:
    # flights — основная факт-таблица с surrogate PK flight_id (integer)
    - schema: "bookings"
      name:   "flights"
      # Режем на чанки по flight_id для параллельной загрузки
      chunk_column: "flight_id"
      chunk_size: 100000
      where: ""
      order_by: "flight_id"
      # 0 = без лимита, переносим всю таблицу
      limit: 0

    # ticket_flights — связка билетов и рейсов
    - schema: "bookings"
      name:   "ticket_flights"
      chunk_column: "flight_id"
      chunk_size: 300000
      where: ""
      order_by: "flight_id"
      # Пример: переносим только первые 100 строк (LIMIT 100).
      # Удобно для тестов/отладки на небольших объёмах.
      limit: 100

    # boarding_passes — посадочные талоны, тоже режем по flight_id
    - schema: "bookings"
      name:   "boarding_passes"
      chunk_column: "flight_id"
      chunk_size: 300000
      where: ""
      order_by: "flight_id"
      limit: 0

    # aircrafts_data — справочник, как правило небольшой, можно не резать
    - schema: "bookings"
      name:   "aircrafts_data"
      chunk_column: ""
      chunk_size: 500000
      where: ""
      order_by: ""
      limit: 0

    # airports_data — справочник аэропортов
    - schema: "bookings"
      name:   "airports_data"
      chunk_column: ""
      chunk_size: 500000
      where: ""
      order_by: ""
      limit: 0

    # seats — информация о местах
    - schema: "bookings"
      name:   "seats"
      chunk_column: ""
      chunk_size: 500000
      where: ""
      order_by: ""
      limit: 0

    # bookings — сами бронирования
    - schema: "bookings"
      name:   "bookings"
      chunk_column: ""
      chunk_size: 500000
      where: ""
      order_by: ""
      limit: 0

    # tickets — билеты
    - schema: "bookings"
      name:   "tickets"
      chunk_column: ""
      chunk_size: 500000
      where: ""
      order_by: ""
      limit: 0

  # Таблицы, которые нужно исключить из миграции.
  # Указываются только при необходимости.
  #
  # Пример:
  #   - schema: "bookings"
  #     name:   "some_big_log_table"
  #
  # В демо-примере ничего не исключаем.
  exclude: []

# ---------------------------------------------------------------------
# 5. Параллелизм
# ---------------------------------------------------------------------

parallel:
  # Максимальное число параллельных задач (JOBS) для GNU parallel.
  #
  # Внутри утилиты итоговое число потоков EXEC_JOBS считается так:
  #   - берём число CPU-ядер: nproc
  #   - считаем 80% от него: max80 = nproc * 0.8
  #   - если max_jobs > 0, то EXEC_JOBS = min(max80, max_jobs)
  #   - если max_jobs = 0, то EXEC_JOBS = max80
  #
  # Примеры:
  #   nproc=16, max_jobs=8  → EXEC_JOBS = 8
  #   nproc=16, max_jobs=32 → EXEC_JOBS = 12 (80% от 16)
  #   nproc=4,  max_jobs=0  → EXEC_JOBS = 3
  max_jobs: 4

# ---------------------------------------------------------------------
# 6. Дополнительные опции
# ---------------------------------------------------------------------

options:
  # Создавать ли индексы после загрузки данных.
  #
  # true  — индексы попадают в post-data и создаются после DATA-задач.
  # false — поведение может быть изменено в будущем (оставлено для расширения).
  create_indexes_after_data: true

  # Создавать ли constraints (foreign key, unique, check) после загрузки данных.
  #
  # В данной версии утилиты этот флаг в основном логируется и зарезервирован
  # под тонкую настройку поведения post-data.
  create_constraints_after_data: false

  # Запускать ли ANALYZE после миграции, чтобы собрать статистику по таблицам.
  #
  # true  — для всех таблиц из плана выполняется ANALYZE.
  # false — статистику оставляем как есть (рассчитываем на autovacuum/autoanalyze).
  analyze_after_load: true
